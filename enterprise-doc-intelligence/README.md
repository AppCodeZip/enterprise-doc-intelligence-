mkdir my_first_python_project
cd my_first_python_project


# 1. Create a virtual environment
python3 -m venv .venv

# 2. Activate it
source .venv/bin/activate

# Create your first Python file
touch main.py

# 3. Upgrade pip (optional but recommended)
pip install --upgrade pip


enterprise-doc-intelligence/
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ loaders.py
â”‚   â”œâ”€â”€ chunker.py
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ cache.py
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ faiss_store.py
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ prompt.py
â”‚   â”œâ”€â”€ qa_chain.py
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/
â”‚
â”œâ”€â”€ README.md


# Code format
black your_file.py

# update library 
pip install -r requirements.txt

<!-- PDF â†’ chunks â†’ embeddings â†’ FAISS â†’ top-K relevant chunks -->




<!-- ğŸ¯ Target Architecture (Simple & Industry-Standard)
User
 â†“
Internet
 â†“
EC2 (Ubuntu)
 â”œâ”€â”€ FastAPI (uvicorn)
 â”œâ”€â”€ FAISS (in-memory)
 â”œâ”€â”€ Ollama / LLM
 â””â”€â”€ Persistent disk (embeddings, index)
1ï¸âƒ£ Create EC2 Instance (AWS)
Step 1: Launch Instance
AMI: Ubuntu 22.04
Instance Type:
CPU only: t3.large
GPU (optional): g4dn.xlarge
Storage: 50 GB
Security Group:
Allow 22 (SSH)
Allow 8000 (FastAPI)
2ï¸âƒ£ SSH Into EC2
ssh -i your-key.pem ubuntu@<EC2_PUBLIC_IP>
3ï¸âƒ£ Install System Dependencies
sudo apt update && sudo apt upgrade -y

sudo apt install -y \
    python3 \
    python3-pip \
    python3-venv \
    git \
    curl
4ï¸âƒ£ Clone Your Project
git clone https://github.com/your-username/enterprise-doc-intelligence.git
cd enterprise-doc-intelligence
5ï¸âƒ£ Python Virtual Environment
python3 -m venv venv
source venv/bin/activate
6ï¸âƒ£ Install Python Dependencies
requirements.txt
(IMPORTANT â€“ create this)
fastapi
uvicorn
faiss-cpu
sentence-transformers
pypdf
python-docx
python-multipart
tqdm
Then install:
pip install -r requirements.txt
7ï¸âƒ£ Install Ollama (LLM Runtime)
curl -fsSL https://ollama.com/install.sh | sh
Pull model:
ollama pull llama3
Test:
ollama run llama3
8ï¸âƒ£ Update FastAPI for Production
Change Uvicorn Command
Instead of --reload (dev only):
uvicorn api.main:app --host 0.0.0.0 --port 8000
9ï¸âƒ£ Run the Server
uvicorn api.main:app --host 0.0.0.0 --port 8000
Open in browser:
http://<EC2_PUBLIC_IP>:8000/docs
ğŸ‰ Your GenAI RAG backend is LIVE on cloud
ğŸ”¥ MAKE IT PRODUCTION-GRADE (VERY IMPORTANT)
10ï¸âƒ£ Run as Background Service (systemd)
Create service file:
sudo nano /etc/systemd/system/rag-api.service
Paste:
[Unit]
Description=Enterprise RAG FastAPI
After=network.target

[Service]
User=ubuntu
WorkingDirectory=/home/ubuntu/enterprise-doc-intelligence
ExecStart=/home/ubuntu/enterprise-doc-intelligence/venv/bin/uvicorn api.main:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target
Enable & start:
sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl enable rag-api
sudo systemctl start rag-api
Check status:
sudo systemctl status rag-api
1ï¸âƒ£1ï¸âƒ£ Persistence (VERY IMPORTANT)
Ensure these files stay on disk:
embeddings_cache.pkl
faiss.index
docs.pkl
They survive restart â†’ no re-indexing
1ï¸âƒ£2ï¸âƒ£ Production Security (Basic)
Put EC2 behind Nginx (optional)
Add API key / JWT
Close port 8000 â†’ expose via Nginx 80/443
(You can add later)
ğŸ§  INTERVIEW: HOW TO EXPLAIN THIS DEPLOYMENT
Say this ğŸ‘‡ (memorize):
â€œI deployed the system on AWS EC2 using a VM-based approach because GenAI workloads require long-running processes, streaming responses, and in-memory vector stores. The FastAPI app runs as a systemd service, FAISS indexes are persisted to disk, and the LLM runs locally via Ollama. This setup avoids cold starts and supports real-time streaming.â€
ğŸ”¥ This is senior-level explanation.
ğŸŒ SAME DEPLOYMENT ON OTHER CLOUDS
Azure
AWS	Azure
EC2	Virtual Machine
S3	Blob Storage
IAM	Azure AD
systemd	same
ğŸ‘‰ Same steps, same commands.
GCP
AWS	GCP
EC2	Compute Engine
S3	Cloud Storage
ALB	Cloud Load Balancer
ğŸ‘‰ Again, same architecture.
ğŸ† FINAL VERDICT
âœ… Your project is cloud-ready
âœ… EC2 deployment is industry-correct
âœ… Interviewers will accept this confidently
âœ… This is real GenAI backend engineering 





1ï¸âƒ£ Docker + ECS Deployment
â“ What it is
You package your FastAPI + RAG system into a Docker image and deploy it on AWS ECS (managed containers).
âœ… What problem it solves
Without Docker	With Docker
â€œWorks on my machineâ€	Same everywhere
Manual setup	Automated
Hard to scale	Easy scaling
OS dependency	Portable
ğŸ’ Why Interviewers LOVE this
When you say Docker + ECS, interviewer hears:
CI/CD readiness
Microservices mindset
Cloud-native engineering
Team-scale deployment
ğŸ—£ï¸ Interview signal:
â€œCandidate understands modern backend deployment.â€
ğŸ”¥ Real-world usage
Almost every production backend
Required for ECS / EKS / Kubernetes
Standard in startups & enterprises
2ï¸âƒ£ Nginx + HTTPS (TLS)
â“ What it is
Nginx acts as:
Reverse proxy
Load balancer
HTTPS terminator
âœ… What problem it solves
Problem	Solution
Exposing port 8000	Use 80/443
No SSL	HTTPS via TLS
Security risk	Industry standard
ğŸ’ Why Interviewers LOVE this
This shows:
You understand networking
You understand security basics
You know how real APIs are exposed
ğŸ—£ï¸ Interview line:
â€œI donâ€™t expose application ports directly; I put them behind Nginx with HTTPS.â€
Thatâ€™s production thinking.
ğŸ”¥ Real-world usage
Every serious API
Mandatory in finance, healthcare, enterprise SaaS
3ï¸âƒ£ S3 Auto-Ingestion (Event-Driven RAG)
â“ What it is
Documents are:
Uploaded to S3
Automatically indexed via events (no manual upload API)
âœ… What problem it solves
Manual Upload	Auto-Ingestion
Human dependent	Fully automatic
Error-prone	Reliable
Not scalable	Scales infinitely
ğŸ’ Why Interviewers LOVE this
This is true automation.
ğŸ—£ï¸ Interview signal:
â€œSystem reacts to data, not humans.â€
This shows:
Event-driven architecture
Asynchronous thinking
Real enterprise workflows
ğŸ”¥ Real-world usage
Knowledge bases
Compliance docs
Internal enterprise search
Legal / HR / Policy systems

â˜ï¸ S3 Auto-Ingestion â€” How It Works (Enterprise Way)
ğŸ¯ Goal (One Line)
Jaise hi koi document S3 me upload hota hai, system automatically usko read, chunk, embed aur index kar deta hai â€” bina kisi manual API call ke.
This is called event-driven ingestion.
ğŸ§  High-Level Flow (Concept)
User / System
   â†“
Upload file to S3 bucket
   â†“
S3 Event Notification (ObjectCreated)
   â†“
Trigger (Lambda OR ECS task)
   â†“
Ingestion Pipeline
   â†“
FAISS / Vector DB Updated
No UI.
No /upload API.
Fully automatic.
ğŸ”¹ Step 1: Document Upload to S3
Who uploads?
Human (HR uploads policy)
System (cron job, CI pipeline)
Another app
Example:
s3://enterprise-docs-bucket/hr/leave_policy.pdf
Thatâ€™s it.
User ka kaam yahin khatam.
ğŸ”¹ Step 2: S3 Event Notification
AWS S3 can emit events like:
ObjectCreated
ObjectRemoved
You configure S3:
â€œWhenever a new file is uploaded â†’ notify somethingâ€
That â€œsomethingâ€ can be:
AWS Lambda âœ… (most common)
SQS
SNS
EventBridge
ğŸ”¹ Step 3: Lambda Triggered Automatically
What Lambda Receives
Lambda gets metadata only (not full file):
{
  "bucket": "enterprise-docs-bucket",
  "key": "hr/leave_policy.pdf"
}
Lambda now knows:
Which file
Where it is
ğŸ”¹ Step 4: Lambda Downloads File from S3
Inside Lambda:
import boto3

s3 = boto3.client("s3")
s3.download_file(bucket, key, "/tmp/leave_policy.pdf")
Now Lambda has the document.
ğŸ”¹ Step 5: Lambda Calls Your RAG Ingestion Logic
Two options (both industry-used):
âœ… Option A: Lambda calls your FastAPI ingestion endpoint
requests.post(
    "http://rag-backend/internal/ingest",
    files={"file": open("/tmp/leave_policy.pdf", "rb")}
)
Your existing logic runs:
load_document
chunk_documents
embed
FAISS add
âœ” Reuse your current code
âœ” Simple
âœ… Option B: Lambda runs ingestion code directly (Better)
Lambda directly imports:
loaders.py
chunker.py
embedder.py
faiss_store.py
This is true automation.
ğŸ”¹ Step 6: FAISS Index Updated
New embeddings added
Metadata stored
Index persisted to disk / S3
Now system is query-ready.
ğŸ”¹ Step 7: User Asks Question (Later)
â€œWhat is the leave policy?â€
System already knows the document â€”
no upload step required.
ğŸ—ï¸ COMPLETE ARCHITECTURE (Mental Picture)
[ S3 Bucket ]
     â†“ (ObjectCreated event)
[ Lambda Function ]
     â†“
[ Ingestion Logic ]
     â†“
[ FAISS Index ]
     â†“
[ FastAPI /query ]
     â†“
[ LLM Answer ]

-->

